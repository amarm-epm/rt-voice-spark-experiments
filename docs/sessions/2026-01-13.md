# Session: 2026-01-13

## Initial Prompt

Build a fully local real-time voice assistant on NVIDIA DGX Spark with Gradio web UI.

User preferences established:
- First milestone: Environment verification
- Interaction model: Push-to-talk via web UI (no physical access to device)
- Models: Use defaults (Whisper, Llama 3.2 8B, Piper)
- UI: Gradio (browser-based audio I/O)

## Work Completed

### Phase 1: Environment Verification ✓

**Tasks completed:**
1. Created `src/` package structure (moved main.py from root)
2. Verified DGX Spark environment:
   - GPU: NVIDIA GB10 (Grace Blackwell)
   - CUDA: 13.0
   - Memory: 119 GB unified
   - CPU: 20-core ARM (Cortex-X925 + A725)
   - OS: Ubuntu 24.04 (aarch64)
3. Documented findings in `docs/ENVIRONMENT.md`
4. Updated `CLAUDE.md` with project guidelines

**Git commits:**
- `a3ef2e1` - Phase 1: Project structure and environment verification

## Next Phase

**Phase 2: Component Setup** (decomposed, core pipeline first)

| Sub-phase | Component | Status |
|-----------|-----------|--------|
| 2.1 | LLM (llama-cpp-python) | ✓ Done |
| 2.2 | STT (Whisper) | ✓ Done |
| 2.3 | TTS (Piper) | ✓ Done |
| 2.4 | UI (Gradio) | Pending |

Each sub-phase: install → create wrapper → test → commit & push

### Phase 2.1: LLM ✓

- Installed `llama-cpp-python` with CUDA backend
- Downloaded Llama 3.2 1B Instruct (Q4_K_M, 771MB)
- Created `src/llm/llama_llm.py` wrapper
- Commit: `d555cb3`

**Metrics:**
| Metric | Value |
|--------|-------|
| Model load time | 0.80s |
| Avg response latency | 0.19s |

**Test results:**
```
User: What is 2 + 2?
Assistant: Two plus two is 4.
Latency: 0.23s

User: Explain gravity in one sentence.
Assistant: Gravity is a natural force that pulls objects with mass towards each other...
Latency: 0.16s

User: How are you today?
Assistant: I'm doing great, thanks for asking! How about you?...
Latency: 0.17s
```

### Phase 2.2: STT ✓

- Installed `faster-whisper` and `openai-whisper`
- Created `src/stt/whisper_stt.py` wrapper with auto-detect device
- Note: PyTorch CUDA not available on aarch64 (using CPU mode)
- Commit: `9a724e6`

**Metrics:**
| Metric | Value |
|--------|-------|
| Model load time | 0.49s |
| Transcription time | 0.75s (~5s audio) |

**Test results:**
```
Input: "Hello, this is a test of the speech recognition system..."
Output: "Hello. This is a test of the speech recognition system. The quick brown fox jumps over the lazy dog."
```

### CUDA Fix: whisper.cpp

**Problem:** PyTorch/CTranslate2 pip wheels don't include CUDA for aarch64.

**Solution:** Built whisper.cpp from source with CUDA support.

```
Device 0: NVIDIA GB10, compute capability 12.1
using CUDA0 backend
BLACKWELL_NATIVE_FP4 = 1
```

**Final Performance (pywhispercpp with CUDA):**
| Metric | Value |
|--------|-------|
| Model load | 0.44s |
| Transcribe (11s audio) | 0.15s |
| Speed | **73x real-time** |

**Comparison:**
| Method | Speed |
|--------|-------|
| faster-whisper (CPU) | 6.7x real-time |
| pywhispercpp (CUDA) | **73x real-time** |
| Improvement | **~11x faster** |

**Solution:** Built pywhispercpp from source with `GGML_CUDA=1`

### Phase 2.3: TTS ✓

- Installed `piper-tts`
- Downloaded en_US-lessac-medium voice (60MB)
- Created `src/tts/piper_tts.py` wrapper
- Commit: `f09abd5`

**Metrics:**
| Metric | Value |
|--------|-------|
| Model load | 0.45s |
| Synthesis (57 chars) | 0.13s |

Note: Running on CPU (onnxruntime CUDA not available for aarch64 pip wheels). Fast enough for real-time.

## Notes

- No container runtime available (Docker/Podman) - using direct pip/uv installation
- Established git workflow: commit after each task, always push to remote
