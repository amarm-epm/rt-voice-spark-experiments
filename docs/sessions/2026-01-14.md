# Session: 2026-01-14

## Summary

Completed the real-time voice assistant MVP on NVIDIA DGX Spark (Grace Blackwell GB10).

## What Was Built

A fully local voice assistant with browser-based UI:

```
[Browser] → Mic Recording → [Whisper STT] → [Llama LLM] → [Piper TTS] → Audio Playback
```

### Components

| Component | Technology | Backend | Performance |
|-----------|------------|---------|-------------|
| STT | pywhispercpp (whisper.cpp) | **CUDA** | 73x real-time |
| LLM | llama-cpp-python | **CUDA** | 0.19s latency |
| TTS | piper-tts | CPU | 0.13s synthesis |
| UI | Gradio | - | Web-based |

### Models

| Component | Model | Size | Details |
|-----------|-------|------|---------|
| STT | Whisper base.en | 142 MB | 74M params, English-only |
| LLM | Llama 3.2 1B Instruct | 771 MB | Q4_K_M quantization |
| TTS | Piper lessac-medium | 61 MB | en_US voice, 22kHz |

**Total models size: ~974 MB**

**Total pipeline latency: ~0.5s**

### Key Files

```
src/
├── main.py              # Entry point
├── run_ssl.py           # HTTPS launcher (self-signed)
├── ui/app.py            # Gradio interface + VoiceAssistant class
├── stt/whisper_stt.py   # Whisper wrapper with auto-resampling
├── llm/llama_llm.py     # Llama wrapper
└── tts/piper_tts.py     # Piper wrapper
```

## Bugs Fixed

1. **Sample rate mismatch**: Whisper expects 16kHz, Piper outputs 22050Hz
   - Solution: Auto-resampling in STT wrapper

2. **Microphone requires HTTPS**: Browser blocks mic over HTTP
   - Tried: Self-signed certificate (doesn't work - not "secure context")
   - Solution: Gradio `share=True` for public HTTPS tunnel

3. **aarch64 CUDA limitations**: PyTorch/CTranslate2 pip wheels are CPU-only
   - Solution: Built whisper.cpp from source with `GGML_CUDA=1`

## How to Run

```bash
# Start the voice assistant
uv run python -m src.main

# Access via public HTTPS URL (printed in console)
# Example: https://84309b116d45d84e0a.gradio.live
```

## Architecture Decisions

- **Gradio share tunnel** for HTTPS (microphone requires secure context)
- **pywhispercpp** instead of faster-whisper (native CUDA on aarch64)
- **CPU for TTS** - fast enough, no CUDA wheels for onnxruntime on aarch64

## Next Steps

- [ ] End-to-end latency optimization
- [ ] Streaming responses (LLM + TTS)
- [ ] Voice activity detection (VAD)
- [ ] Conversation history/context
